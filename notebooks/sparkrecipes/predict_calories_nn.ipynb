{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMDB_FILEPATH_TRAIN = \"/mnt/lmdb_storage/seefood_train_data\"\n",
    "LMDB_FILEPATH_TEST = \"/mnt/lmdb_storage/seefood_test_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import lmdb\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(name, y_test, y_pred):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"experiment_name\": name,\n",
    "            \"r2_score\": [r2_score(y_test, y_pred)],\n",
    "            \"explained_variance_score\": [explained_variance_score(y_test, y_pred)],\n",
    "            \"max_error\": [max_error(y_test, y_pred)],\n",
    "            \"mean_absolute_error\": [mean_absolute_error(y_test, y_pred)],\n",
    "            \"mean_squared_error\": [mean_squared_error(y_test, y_pred)],\n",
    "            \"median_absolute_error\": [median_absolute_error(y_test, y_pred)],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bigquery df_nn --project zenscr-seefood-dev\n",
    "\n",
    "SELECT title, image_path, total_calories\n",
    "FROM `zenscr-seefood-dev.sparkrecipes.base_filtered`\n",
    "INNER JOIN `zenscr-seefood-dev.sparkrecipes.image_path`\n",
    "USING (recipe_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_nn.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_ = df_nn.total_calories.hist(bins=150, figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "\n",
    "class Features:\n",
    "    def __init__(self, features, target):\n",
    "        self.shape = features.shape\n",
    "        self.features = features.numpy().tobytes()\n",
    "        self.target = target.round().item()\n",
    "\n",
    "    def get_features(self):\n",
    "        features = np.frombuffer(self.features, dtype=np.float32)\n",
    "        return torch.from_numpy(features.reshape(self.shape))\n",
    "\n",
    "\n",
    "class LMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lmdb_filename):\n",
    "        self.env = lmdb.open(\n",
    "            lmdb_filename,\n",
    "            max_readers=1,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "        )\n",
    "        print(self.env.stat())\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = txn.stat()[\"entries\"]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            key = f\"{index:08}\".encode(\"ascii\")\n",
    "            buf = txn.get(key)\n",
    "\n",
    "        features = pickle.loads(buf)\n",
    "        return features.get_features(), features.target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class CalorieNet(nn.Module):\n",
    "    \"\"\" Predicts calories given an image displaying food \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CalorieNet, self).__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(0.2), nn.Linear(1280, 512), nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.mean([2, 3])\n",
    "        y = self.regressor(x)\n",
    "        y = y.squeeze()\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psize': 4096, 'depth': 3, 'branch_pages': 27, 'leaf_pages': 5671, 'overflow_pages': 54489506, 'entries': 878863}\n",
      "{'psize': 4096, 'depth': 3, 'branch_pages': 14, 'leaf_pages': 2793, 'overflow_pages': 26838188, 'entries': 432874}\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        LMDBDataset(LMDB_FILEPATH_TRAIN),\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    ),\n",
    "    \"val\": torch.utils.data.DataLoader(\n",
    "        LMDBDataset(LMDB_FILEPATH_TEST),\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CalorieNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    writer = SummaryWriter()\n",
    "    since = time.time()\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            i = 0\n",
    "            for inputs, targets in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            if phase == \"train\":\n",
    "                training_loss.append(epoch_loss)\n",
    "                writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "            else:\n",
    "                validation_loss.append(epoch_loss)\n",
    "                writer.add_scalar(\"Loss/val\", epoch_loss, epoch)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print(f\"Training complete in {time_elapsed/60}m {time_elapsed % 60}s\")\n",
    "    writer.close()\n",
    "    return model, (training_loss, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd7e8bb699c443f8076536d1e61c834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f87ef1e7e946b59016c9af847526a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13733.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.87162 s\n",
       "File: <ipython-input-88-a9b305f5875d>\n",
       "Function: train_model at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
       "     2         1       2509.0   2509.0      0.0      writer = SummaryWriter()\n",
       "     3         1          3.0      3.0      0.0      since = time.time()\n",
       "     4         1          1.0      1.0      0.0      training_loss = []\n",
       "     5         1          1.0      1.0      0.0      validation_loss = []\n",
       "     6                                           \n",
       "     7         1      36856.0  36856.0      0.5      for epoch in tqdm(range(num_epochs)):\n",
       "     8         1          3.0      3.0      0.0          for phase in [\"train\", \"val\"]:\n",
       "     9         1          1.0      1.0      0.0              if phase == \"train\":\n",
       "    10         1         85.0     85.0      0.0                  model.train()\n",
       "    11                                                       else:\n",
       "    12                                                           model.eval()\n",
       "    13         1          1.0      1.0      0.0              running_loss = 0.0\n",
       "    14                                           \n",
       "    15         1          1.0      1.0      0.0              i = 0\n",
       "    16       356    6019045.0  16907.4     76.5              for inputs, targets in tqdm(dataloaders[phase]):\n",
       "    17       355    1236317.0   3482.6     15.7                  inputs = inputs.to(device)\n",
       "    18       355      19073.0     53.7      0.2                  targets = targets.to(device)\n",
       "    19                                           \n",
       "    20       355      30290.0     85.3      0.4                  optimizer.zero_grad()\n",
       "    21       355       4444.0     12.5      0.1                  with torch.set_grad_enabled(phase == \"train\"):\n",
       "    22       355     139254.0    392.3      1.8                      outputs = model(inputs)\n",
       "    23       355      47289.0    133.2      0.6                      loss = criterion(outputs, targets)\n",
       "    24       355        579.0      1.6      0.0                      if phase == \"train\":\n",
       "    25       355     231153.0    651.1      2.9                          loss.backward()\n",
       "    26       355      87261.0    245.8      1.1                          optimizer.step()\n",
       "    27       355      17453.0     49.2      0.2                  running_loss += loss.item() * inputs.size(0)\n",
       "    28                                           \n",
       "    29                                                       if phase == \"train\":\n",
       "    30                                                           scheduler.step()\n",
       "    31                                           \n",
       "    32                                                       epoch_loss = running_loss / dataset_sizes[phase]\n",
       "    33                                                       if phase == \"train\":\n",
       "    34                                                           training_loss.append(epoch_loss)\n",
       "    35                                                           writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
       "    36                                                       else:\n",
       "    37                                                           validation_loss.append(epoch_loss)\n",
       "    38                                                           writer.add_scalar(\"Loss/val\", epoch_loss, epoch)\n",
       "    39                                           \n",
       "    40                                               time_elapsed = time.time() - since\n",
       "    41                                           \n",
       "    42                                               print(f\"Training complete in {time_elapsed/60}m {time_elapsed % 60}s\")\n",
       "    43                                               writer.close()\n",
       "    44                                               return model, (training_loss, validation_loss)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    }
   ],
   "source": [
    "%lprun -f train_model model, metrics = train_model(net, criterion, optimizer, exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss, validation_loss = metrics\n",
    "pd.DataFrame({\"training\": training_loss, \"validation\": validation_loss}).plot.line(\n",
    "    figsize=(15, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataloader = torch.utils.data.DataLoader(\n",
    "    ImageDataset(X_nn_val, y_nn_val, data_transforms[\"val\"]),\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = []\n",
    "for inputs, _ in predict_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    y_pred_nn.append(model(inputs).to(cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn_np = torch.flatten(torch.cat(y_pred_nn)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn_results = get_metrics(\"nn\", y_nn_val, y_pred_nn_np)\n",
    "df_nn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "class BaselineModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, _, y):\n",
    "        self.mean_ = y.mean()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, [])\n",
    "\n",
    "        return np.array(X.shape[0] * [self.mean_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fit(X_nn_train, y_nn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = baseline_model.predict(X_nn_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_results = get_metrics(\"baseline\", y_nn_val, y_pred_baseline)\n",
    "df_baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare NN to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.concat([df_baseline_results, df_nn_results]).reset_index(drop=True).T\n",
    "df_results.columns = df_results.loc[\"experiment_name\"].values\n",
    "df_results = df_results.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.plot.bar(log=True, figsize=(12, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (0, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn.loc[X_nn_train.index].total_calories.plot.hist(\n",
    "    bins=300, figsize=(16, 11), ylim=ylim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_nn.loc[X_nn_val.index].assign(\n",
    "    predicted_calories=np.exp(y_pred_nn_np)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions[[\"total_calories\", \"predicted_calories\"]].plot.hist(\n",
    "    bins=300, figsize=(16, 11), alpha=0.8, ylim=ylim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_sample = df_predictions\n",
    "line = (\n",
    "    alt.Chart(df_predictions_sample)\n",
    "    .mark_line()\n",
    "    .encode(x=\"total_calories\", y=\"total_calories\")\n",
    ")\n",
    "\n",
    "scatter = (\n",
    "    alt.Chart(df_predictions_sample)\n",
    "    .mark_circle(color=\"red\")\n",
    "    .encode(\n",
    "        x=\"total_calories\",\n",
    "        y=\"predicted_calories\",\n",
    "        tooltip=[\"title\", \"total_calories\", \"predicted_calories\"],\n",
    "    )\n",
    ").interactive()\n",
    "\n",
    "(line + scatter).properties(width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_FEATURE_COLS = [\"title\", \"total_calories\", \"servings\", \"predicted_calories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_cal = df_predictions[df_predictions.predicted_calories > 300]\n",
    "df_low_cal = df_predictions[df_predictions.predicted_calories < 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_cal[NON_FEATURE_COLS].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_cal[NON_FEATURE_COLS].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_cal[NON_FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_cal[NON_FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cal_wc = WordCloud().generate(\" \".join(df_high_cal.title.str.lower()))\n",
    "low_cal_wc = WordCloud().generate(\" \".join(df_low_cal.title.str.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(high_cal_wc, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(low_cal_wc, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, \"../../models/calorie_regression_mobilenet_.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
